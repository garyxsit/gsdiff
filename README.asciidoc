= gsdiff

`gsdiff` is a diff tool that works across data stores through a global scope.

Suppose the organization has a data lake that comprises various kinds of data stores (e.g. databases, https://www.ibm.com/cloud/learn/data&#x002D;warehouse#toc&#x002D;what&#x002D;is&#x002D;a&#x002D;&#x002D;1NvW7Mkj[data warehouses], files, and directory services) and possibly cloud data lakes. All of these data stores have their own formats. Therefore, the data lake would be facing a common scenario that the shared data would need to be stored in one centralized location for regulatory compliance reasons.

Take these records from data stores as an example: (For simplicity, a data munging process is at first performed to transform them into JSON-LD as shown below)
[source, javascript]
----
{
  "@context":
  {
    "@id": "http://xmlns.com/foaf/spec/Organization",
    "@type": "@id",
    "member": [
      {
        "@id": "http://xmlns.com/foaf/spec/Person",
        "@type": "@id",
        "name": "Gary Sit",
        "mbox": "mailto:garyxsit@gmail.com",
        "phone": "tel:+852.23456789",
        "interest": "http://en.wikipedia.org/wiki/Semantic_Web"
      }
    ]
  }
}

----
[source, javascript]
----
{
  "@context":
  {
    "@id": "http://schema.org/Organization",
    "@type": "@id",
    "member": [
      {
        "@id": "http://schema.org/Person",
        "@type": "@id",
        "name": "Gary Sit",
        "email": "mailto:garyxsit@gmail.com",
        "telephone": [
                       "+852 12345678",
                       "+1-617-253-1000"
                     ],
        "address":
        {
          "@type": "PostalAddress",
          "addressLocality": "Hong Kong",
          "addressRegion": "HK"
        }
      }
    ]
  }
}
  
----

These 2 data records differ in the following:

. The FOAF one has an extra property `interest`
. The schema.org one has an extra property `address`
. The property names (`mbox` for FOAF &#x003C;&#x003D;&#x003E; `email` for schema.org & `phone` for FOAF &#x003C;&#x003D;&#x003E; `telephone` for schema.org) and their values (tel: scheme for FOAF &#x003C;&#x003D;&#x003E; formatted strings for schema.org) are different
. The schema.org one included a new phone number
. Either one of the actual data isn&#x2019;t updated (e.g. an employee in the organization switched to use another phone number in a country)

This tool solves the above problem by comparing data records between data sources to recommend data discrepancies using a familiar SQL syntax with a similarity measure:
[source, sql]
----
SELECT * FROM origin
WHERE get_similarity(Oracle.metadata, Cassandra.metadata) >= 0.95

----

This similarity measure uses statistical models (in the domain of Data Mining &#x2229; Machine Learning &#x2229; Deep Learning &#x2229; Data Science) to compare 2 inputs and generate a report in YAML when they&#x2019;re highly similar. Here&#x2019;s the sample output:
[source, yaml]
----
version: v1
discrepancies:
  variations:
    - property:
        item1:
          name:  "@context.@id" # equals item2.name
          value: http://xmlns.com/foaf/spec/Organization
        item2:
          name:  "@context.@id" # equals item1.name
          value: http://schema.org/Organization
    - property:
        item1:
          name:  "@context.member[:1].@id" # equals item2.name
          value: http://xmlns.com/foaf/spec/Person
        item2:
          name:  "@context.member[:1].@id" # equals item1.name
          value: http://schema.org/Person
    - property:
        item1:
          name:  "@context.member[:1].mbox"
          value: mailto:data.masked@gmail.com # equals item2.value
        item2:
          name:  "@context.member[:1].email"
          value: mailto:data.masked@gmail.com # equals item1.value
    - property:
        item1:
          name:  "@context.member[:1].phone"
          value: tel:+*5*.*3*5*7*9 # data are masked
        item2:
          name:  "@context.member[:1].telephone"
          value: [
                   +*5* *2*4*6*8,  # data are masked
                   +1-*1*-*5*-*0*0 # data are masked
                 ]
  additions:
    - property:
        item1:
          name:  "@context.member[:1].interest"
          value: http://en.wikipedia.org/wiki/Semantic_Web
        item2:
          name:  "@context.member[:1].address"
          value: {
                   "@type": PostalAddress,
                   addressLocality: Hong Kong,
                   addressRegion: HK
                 }

----

The values of telephone and address properties are essentially in JSON formats, and thus they can be embedded in YAML as the output.

Using this tool enables users to focus on business activities (from data wrangling & data masking to encryption processes) instead of manually searching for common data models across data stores in the organization.

Currently it supports these categories:

* RDBMS (MySQL, PostgreSQL, Oracle, Microsoft SQL Server)
* NoSQL (Cassandra, MongoDB, Elasticsearch)
* Data Warehouse (Apache Hive)
* Tabular data formats (.csv, .tsv) and Excel formats (.xls, .xlsx)
* Directory Service (Active Directory, Azure AD DS)

The source code will be provided upon request if interested.

== Quick Q&A Excerpts
The FOAF namespace is "http://xmlns.com/foaf/0.1/".:::
That&#x2019;s true. These JSON-LD @id keywords also recognize an IRI including the namespace, although it sounds it must use the URL related to the namespace at first glance.

Data warehouses don&#x2019;t belong to the data store and they&#x2019;re still competing against data lakes.:::
It depends on the context. Please read https://www.ibm.com/cloud/learn/data&#x002D;warehouse#toc&#x002D;what&#x002D;is&#x002D;a&#x002D;&#x002D;1NvW7Mkj[the data warehouse description from IBM]: +
The data store in general has several meanings. Its narrow description can be an Operational Data Store instance, which is in the data warehouse, and its broad description can be a category of a logical data storage representation from data storage devices, and thus a data warehouse would be in the data store category in this sense. +
Sometimes it might be interpreted as the data lake has a data warehouse, but it doesn&#x2019;t indicate there exists a transitive relation in this case. Just like a situation that even though Alice has a child named Bob and there is a child named Carol, it doesn&#x2019;t mean Alice has Carol as her child. +
As for data lakes, probably because many reports described these 2 terms for comparisons (the data warehouse vs. the data lake) and arouse a misunderstanding that they are mutually exclusive. These, however, aren&#x2019;t implying only one of them can be used. In practice, most of the modern cloud architectures involve a data lake as the data ingestion layer in front of a data warehouse for buffering because of the nature of using the secondary storage in a data warehouse for archival purposes. +
To sum up, Wikipedia doesn&#x2019;t represent everything and don&#x2019;t judge before knowing the known unknowns and the unknown unknowns.

Syntax errors in your SQL.:::
The code snippet is a string having SQL syntactic keywords (not yet for binding semantics) passed from an actual parameter to a formal parameter that can be parsed by the tool for the processing. It is relatively simpler than one of the corresponding SQL representations:
[source, sql]
----
SELECT * FROM origin
WHERE get_similarity(Oracle.metadata, Cassandra.metadata) >= 0.95

----
[source, sql]
----
SELECT * FROM origin
FULL OUTER JOIN Oracle.metadata ON origin.id = Oracle.metadata.id
FULL OUTER JOIN Cassandra.metadata ON Oracle.metadata.id = Cassandra.metadata.id
WHERE get_similarity(Oracle.metadata, Cassandra.metadata) >= 0.95

----
Ok.:::
The idea is to reuse SQL keywords as an input and then use graph processing libraries for traversing metadata, and thus it is natural to use these combinations to handle large amounts of data for comparisons at the same time. +
*Update:* Just wrote a primitive transpiler to translate the code based on type inference mechanisms so the multi-table join SQL also works now.

SQL cannot “elegantly” handle traversing a graph.:::
As of 2017, that was true. It is because the SQL semantics in the matter of functions don&#x2019;t own sufficient vocabularies to express all of the graph processing capabilities. It doesn&#x2019;t mean traversal operations cannot be built on the SQL layer. The connections between SQL syntaxes and graph semantics in the perspective of functions become relatively similar and can be mapped correspondingly when the grammar does not take its context into account. Therefore, several products have been available in the market, although they tend to create their keywords as SQL extensions eventually. After all, it is already 2020 now.

You mentioned there are differences between syntaxes and semantics from the input. In which cases, would they be the same without concerning the context?:::
That is a metaphor and we all know the corresponding meaning in their literal uses are different. However, there is at least one case that would make them appear the same. One of the cases is most of the existing databases would report semantic errors as syntax errors in error messages. It could then be one of the possibilities that this would be interpreted as semantics are equal to syntaxes. Another one is when searching the `synonyms for syntactic` on the web and the word `semantic` will be listed as one of the synonym items of the `syntactic`. They are comparatively similar in these conditions since the actual meaning of the word `synonymous` is close to similar, but neither equal nor identical.

What you said is a simile rather than a metaphor.:::
It is a metaphor since the phrase `as long as` does not mean nearly in the same length as in a simile literally. It is true that this can be interpreted as another meaning as phrases are frequently referred to a meaning that is different from its literal meaning word by word, and thus it is rephrased and should be clarified now.

The report doesn&#x2019;t output discrepancies for @id.:::
There will be collision issues on the property name if it includes @id and uses the previous scheme. Therefore, it is now updated to use an expression based on JSONPath as the string instead.

How do you handle data sources without data headers?:::
Using Data Mining on the content and its structure of a data record since the tool will read the data in the matrix form anyway. After getting the above information it in turn uses the aforementioned similarity measure to advise and classify the data into a type based on the knowledge base of a posteriori (e.g. column headers are combined as a type of the personal information) as this process would make post-processings efficient.
