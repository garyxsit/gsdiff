= gsdiff

`gsdiff` is a diff tool that works across data stores through a global scope.

Suppose the organization has a data lake that comprised of various kinds of data stores (e.g. databases, https://www.ibm.com/cloud/learn/data&#x002D;warehouse#toc&#x002D;what&#x002D;is&#x002D;a&#x002D;&#x002D;1NvW7Mkj[data warehouses], files, and directory services) and possibly cloud data lakes. All of these data stores have their own formats. Therefore, the data lake would be facing a common scenario that the shared data would need to be stored in one centralized location for regulatory compliance reasons.

Take these records from data stores as an example: (For simplicity, a data munging process is at first performed to transform them into JSON-LD as shown below)
[source, javascript]
----
{
  "@context":
  {
    "@id": "http://xmlns.com/foaf/spec/Organization",
    "@type": "@id",
    "member": [
      {
        "@id": "http://xmlns.com/foaf/spec/Person",
        "@type": "@id",
        "name": "Gary Sit",
        "mbox": "mailto:garyxsit@gmail.com",
        "phone": "tel:+852.23456789",
        "interest": "http://en.wikipedia.org/wiki/Semantic_Web"
      }
    ]
  }
}

----
[source, javascript]
----
{
  "@context":
  {
    "@id": "http://schema.org/Organization",
    "@type": "@id",
    "member": [
      {
        "@id": "http://schema.org/Person",
        "@type": "@id",
        "name": "Gary Sit",
        "email": "mailto:garyxsit@gmail.com",
        "telephone": [
                       "+852 12345678",
                       "+1-617-253-1000"
                     ],
        "address":
        {
          "@type": "PostalAddress",
          "addressLocality": "Hong Kong",
          "addressRegion": "HK"
        }
      }
    ]
  }
}
  
----

These 2 data records differ in the following:

. The FOAF one has an extra property `interest`
. The schema.org one has an extra property `address`
. The property names (`mbox` for FOAF &#x003C;&#x003D;&#x003E; `email` for schema.org & `phone` for FOAF &#x003C;&#x003D;&#x003E; `telephone` for schema.org) and their values (tel: scheme for FOAF &#x003C;&#x003D;&#x003E; formatted strings for schema.org) are different
. The schema.org one included a new phone number
. Either one of the actual data isn&#x2019;t updated (e.g. an employee in the organization switched to use another phone number in a country)

This tool solves the above problem by comparing data records between data sources to recommend data discrepancies using a familiar SQL syntax with a similarity measure:

[source, sql]
----
SELECT * FROM origin
WHERE get_similarity(Oracle.metadata, Cassandra.metadata) >= 0.95

----

This similarity measure uses Data Mining/Machine Learning/Deep Learning/Data Science models to compare 2 inputs and generate a report in YAML when they&#x2019;re highly similar. Here&#x2019;s the sample output:
[source, yaml]
----
version: v1
discrepancies:
  variations:
    - property:
        item1:
          name:  mbox
          value: mailto:data.masked@gmail.com # equals item2.value
        item2:
          name:  email
          value: mailto:data.masked@gmail.com # equals item1.value
    - property:
        item1:
          name:  phone
          value: tel:+*5*.*3*5*7*9 # data are masked
        item2:
          name:  telephone
          value: [
                   +*5* *2*4*6*8,  # data are masked
                   +1-*1*-*5*-*0*0 # data are masked
                 ]
  additions:
    - property:
        item1:
          name:  interest
          value: http://en.wikipedia.org/wiki/Semantic_Web
        item2:
          name:  address
          value: {
                   "@type": PostalAddress,
                   addressLocality: Hong Kong,
                   addressRegion: HK
                 }

----

The values of telephone and address properties are essentially in JSON formats, and thus they can be embedded in YAML as the output.

Using this tool enables users to focus on business activities (from data wrangling & data masking to encryption processes) instead of manually searching for common data models across data stores in the organization.

Currently it supports these categories:

* RDBMS (MySQL, PostgreSQL, Oracle, Microsoft SQL Server)
* NoSQL (Cassandra, MongoDB, Elasticsearch)
* Tabular data formats (.csv, .tsv) and Excel formats (.xls, .xlsx)

The source code will be provided upon request if interested.

== Quick Q&A Excerpts
The FOAF namespace is "http://xmlns.com/foaf/0.1/".:::
It is. These JSON-LD @id keywords also recognize an IRI including the namespace, although it sounds it must use the URL related to the namespace at first glance.

Data warehouses don&#x2019;t belong to the data store and they&#x2019;re still competing against data lakes.:::
It depends on the context. Please read https://www.ibm.com/cloud/learn/data&#x002D;warehouse#toc&#x002D;what&#x002D;is&#x002D;a&#x002D;&#x002D;1NvW7Mkj[the data warehouse description from IBM]: +
The data store in general has several meanings. Its narrow description can be an Operational Data Store instance, which is in the data warehouse, and its broad description can be a category of a logical data storage representation from data storage devices, and thus a data warehouse would be in the data store category in this sense. +
Sometimes it might be interpreted as the data lake has a data warehouse, but it doesn&#x2019;t indicate they are in a transitive relationship in this case. Just like a situation that even though Alice has a child named Bob and there is a child named Carol doesn&#x2019;t mean Alice has Carol as her child. +
As for data lakes, probably because many reports described these 2 terms for comparisons (the data warehouse vs. the data lake) and arouse a misunderstanding that they are mutually exclusive. These, however, aren&#x2019;t implying only one of them can be used. In practice, most of the modern cloud architectures involve a data lake as the data ingestion layer in front of a data warehouse for buffering because of the nature of using the secondary storage in a data warehouse. +
To sum up, Wikipedia doesn&#x2019;t represent everything and don&#x2019;t judge before knowing the known unknowns and the unknown unknowns.

Syntax errors in your SQL.:::
The code snippet is a string having SQL syntactic keywords (not yet for binding semantics) used to be passed as a parameter that the tool can parse it for processing. It is relatively simpler than one of corresponding SQL representations:
[source, sql]
----
SELECT * FROM origin
WHERE get_similarity(Oracle.metadata, Cassandra.metadata) >= 0.95

----
[source, sql]
----
SELECT * FROM origin
FULL OUTER JOIN Oracle.metadata ON origin.id = Oracle.metadata.id
FULL OUTER JOIN Cassandra.metadata ON Oracle.metadata.id = Cassandra.metadata.id
WHERE get_similarity(Oracle.metadata, Cassandra.metadata) >= 0.95

----
Ok.:::
The idea is to reuse SQL keywords as an input and then use graph processing libraries for traversing metadata, and thus it is natural to use these combinations to handle large amounts of data for comparisons at the same time.

SQL cannot “elegantly” handle traversing a graph.:::
As of 2017, it was. It is because the SQL semantics don&#x2019;t own sufficient vocabularies to express all of the graph processing capabilities. It doesn&#x2019;t mean traversal operations cannot be built on the SQL layer. Syntaxes and semantics become synonymous as long as the grammar is context-free. Therefore, several products have been available in the market although they tend to create their keywords as SQL extensions eventually. After all, it&#x2019;s 2020 now and already started a new decade...

The report doesn&#x2019;t output discrepancies for @id.:::
Agreed after having the second thought and it will be fixed.

How do you handle data sources without data headers?:::
Using Data Mining on the content and its structure of a data record since the tool will read the data in the matrix form anyway. After getting the above information it in turn uses the aforementioned similarity measure to advise and classify the data into a type based on the knowledge base of a posteriori (e.g. column headers are combined as a type of the personal information) as this process would make post-processings efficient.
